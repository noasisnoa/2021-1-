# -*- coding: utf-8 -*-
"""딥러닝 입문 05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SNdj_lRSBHN8AOD4n6z8C7j5NliGHwRb

# 훈련 노하우를 배웁니다.

숲을 봐라/ 전체적인 구조에 대한 이해

위스콘신 유방안 데이터 세트를 두 덩어리로 나눈 '훈련 세트'와 '테스트 세트'를 준비
훈련 세트 fit()메서드에 전달되어 모델을 훈련하는 데 사용하였고, 테스트 세트는 score()메서드에 전달해 모델의 성능을 평가평가했죠. 여기서는 '테스트 세트'의 사용 방법에 대해 조금 더 깊이 알아보려 합니다. 목표는 '어느 데이터 세트에만 치우친 모델을 만들지 않는 것'입니다.
"""

import numpy as np
import matplotlib.pyplot as plt

"""## 테스트 세트로 모델을 튜닝한다."""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
x_train_all , x_test, y_train_all, y_test = train_test_split(x, y, stratify = y, test_size =0.2, random_state=42)
#split 나누다.
# stratify 계층화하다.
#test_size 는 1최대

## 이게 뭔지 모르겠습니다.
from sklearn.linear_model import SGDClassifier

sgd = SGDClassifier(loss='log', random_state=42)
sgd.fit(x_train_all, y_train_all)
sgd.score(x_test, y_test)

sgd = SGDClassifier(loss='hinge', random_state=42)
sgd.fit(x_train_all, y_train_all)
sgd.score(x_test, y_test)

x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)

# 책에 내용 찾아봐욤

print(x_train.shape, x_val.shape)

print(len(x_train), len(x_val))

# x_train, y_train
# x_val, y_val
sgd = SGDClassifier(loss='log', random_state=42)
sgd.fit(x_train, y_train)
sgd.score(x_val, y_val)

print(cancer.feature_names[[2, 3]]) #
plt.boxplot(x_train[:, 2:4])
plt.xlabel('feature')
plt.ylabel('value')
plt.show()

"""훈련세트가 준비되었으니 본격적으로 로지스킥 회귀를 구현해보자
정방향으로 데이터가 플러가는 과정과 가중치를 업데이트하기 위해 역방향으로 데이터가 흘러가는 과정을 구현해야 한다.

"""

class LogisticNeuron: 
    def __init__(self):
        self.w = None
        self.b = None
        
    def forpass(self , x):
        z = np.sum(x * self.w) + self * b # 직선방향을 계산한다. # 왜 더하는 걸까?
        return z
    
    def backprop(self, x, err):# err가 backprop에 들어가는 내용인가?
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad

# activation()메서드 구현하기
# 시그모이드 함수가  사용되어야 한다. 시그모이드 함수는 자연 상수의 지수 함수를 계산하는 넘파이의 np.exp()함수를 사용

def avrivation(self, z):
    z = np.clip(z, -100, 100)
    a = 1 / (1 + np.exp(-z)) # 시그모이드 계산 # np.exp는 자연상수의 지수함수를 계산한다
    return a


# 훈련하는 메서드 구현하기

def fit(self, x, y, epochs=100):
    self.w = np.ones(x.shap[1]) # 앞에서 초기화 하지 않은 가중치는 np.ones()의 함수를 이용하여 간단히 1로 초기화하고
    self.b = 0                  # 절편은 0으로 초기화한다.
    for i in range (epochs):
        for x_i, y_i in zip(x, y):
            z = self.forpass(x_i) # 정방향 계산
            a = self.activation(z)# 활성화 함수 적용 # 활성화 함수 다시 한번 확인해 보기
            err = -(y_i - a )     # 오차 계산
            w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산
            self.w -= w_grad # 가중치 업데이트
            self.b -= b_grad # 절편 업데이트

# predict()메서드 구현하기
def predict(self, x):
    z = [self.forpass(x_i) for x_i in x]  # 선형함수 적용
    a = self.activation(np.arry(z))       # 활성화 함수 적용
    return a >0.5                         # 계단 함수 적용

neuron = LogisticNeuron()
neuron.fit(x_train, y_trian)

np.mean(neuron.predict(x_test) == y_test)

class SingleLayer:
    def __init__(self, learning_rate = 0.1, l1=0, l2=0): #self 매개변수, learing_rate:매개변수, learining_rate와 l1과 l2는 왜 괄호 안에 들어간 것일까?
        self.w = None
        self.b = None
        self.losses = []         # 오차 err와 차이가 먼가요? 전역변수? 시그모이드?/ 역방향 계산을 하는 이유는? 그래디언트? 클래스와 페키지가 어떻게 다른가?
        self.val_losses = []     # 검증 손실
        self.w_history = []      # 가중치 히스토리 
        self.lr = learning_rate  # 학습률
        self.l1 = l1
        self.l2 = l2
    
    def forpass(self, x):
        z = np.sum(x*self.w) + self.b # 직선 방정식을 계산합니다.
        return z
    
    def backprop(self, x, err):
        w_grad = x * err     # 가중치에 대한 그래디언트를 계산한다.
        b_grad = 1 * err     # 절편에 대한 그래디언트를 계산한다.
        return w_grad, b_grad
    
    def activation(self, z):
        z = np.clip(z, -100, None) # 안정한 np.exp() 계산을 위해
        a = 1 / (1 + np.exp(-z)) #시그모이드 계산
        return a
    
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):