# -*- coding: utf-8 -*-
"""딥러닝 입문 04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khsCErK9ZXJWsHwNi1jY7F0svzS7_kGm

### 아달린(Adaline)에 대해서 알아보자

- 터셉트론을 개선한 적응형 선형 뉴런(Adaptive Linear Neuron)
- 선형 함수의 결과를 학습에 사용한다. 
- 역방향 계산이 계단함수 출력 이후에 일어나지 않고 선현 함수 출력 이후에 진행된다. 

로지스틱 회귀는 이달린의 개선 버전

### 로지스틱 회귀(logistic regression)에 대해 알아보자
![](../Data/041.png)

로지스틱 회귀는 선형 함수를 통과시켜 얻은 z를 임계함수에 보내기 저넹 변형시킨다, 이런 함수를 활성화 함수(activation function)이라고 부른다.

- 활성화 함수를 통과한 값이 a로 표현된다.

로지스틱은 마지막 단계에서 임계함수를 사용하여 예측을 수행한다.임계함수는 활성화 함수의 출력값을 사용한다는 점이 계단함수와 다르다.

활성화 함수는 비선형 함수(Activation function)를 사용한다.

## 04-2 시그모이드 함수로 확률을 만듭니다.

### 시그모이드 함수의 역활을 알아봅니다.
로지스틱 회귀의 전체 구조를 한 번 더 살펴보면서 시그모이드 함수가 어떤 역활을 하는지 알아보자

![](../Data/041.png)

가장 왼쪽에 있는 뉴런이 선형 함수리고 선형 함수의 출력값은 z이다. 
z는 활성화 함수를 통과하여 a가 된다. 이때 로지스틱 회귀에서 사용하는 화성화 함수인 시그모이드 함수는 z를 0~1ㄹ 사이의 확률값으로 변환해 준다.

![](../Data/042.png)


### 시그모이드 함수가 만들어 지는 과정

## 로지스틱은 '이진분류를 위한 알고리즘'
우리는 아직 가중치와 절편을 적절하게 업데이트하는 방법을 배우지 않았다. 선형함수에서에서 손실 함수로 제곱오차를 사용했듯이 분류 문제에서도 제곱 오차를 사용할 수 있을까요? 이제는 로지스틱 회귀를 위한 손실 함수인 로지스틱 손실함수레 대해 알아보겠습니다.

## 04-3로지스틱 손실함수를 경사하강법에 적용한다.
"""