# -*- coding: utf-8 -*-
"""딥러닝 입문 03-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FT7NpehdHIBnwSCTS1GFBcyKulynHS_1

# 03. 머신러닝의 기초를 다집니다.

03-1 선형회귀에 대해 알아봅니다.
"""

from sklearn.datasets import load_diabetes
diabetes = load_diabetes()

print(diabetes.data.shape, diabetes.target.shape)

diabetes.data[0:3]

diabetes.target[:3]

"""당뇨병 환자 데이터 시각화하기"""

import matplotlib.pyplot as plt

plt.scatter(diabetes.data[:, 2], diabetes.target)
plt.xlabel('x')
plt.ylabel('y')
plt.show()

x = diabetes.data[:, 2]
y = diabetes.target

# 퀴즈

x_sample = x[99:109]
# 데이터의 크기 확인해 보기
print(x_sample, x_sample.shape)

"""# 경사하강법에 대해 알아봅니다

03-1 산점도에서 가장 잘 맞는 직선을 그린다면 어떤 모습일까?

예측값과 변화율에 대해 알아보자

## 예측값으로 올바른 모델 찾기
"""

# 1. 임의의 값으로 W와 b 초기화 하기

w = 1.0
b = 1.0

# 2. 훈련 데이터의 첫 번째 샘플 데이터로 Y 얻기
#.  - 첫 번째 샘프에 대한 예측 만들기
y_hat = x[0] * w + b
print(y_hat)

# 3. 타깃과 예측데이터 비교하기
#   - 첫 번째 샘플의 실제 타깃
print(y[0])

"""첫 번째 샘플값에 임의로 w와 Y의 값을 예측해보면 큰 차이가 난다."""

# 4. w값 조절해 예측값 바꾸기
w_inc = w + 0.1
y_hat_inc = w_inc * x[0] + b
print(y_hat_inc)

#5. w값 조정한 후에 예측값 증가 정도 확인하기
#w가 0.1만큼 증가했을 때 y_hat의 값이 얼마나 증가했는지 계산해보자

# y_hat 이 증가한 양을 w가 증가한 양으로 나누자
w_rate = (y_hat_inc - y_hat) / (w_inc - w) 
print(w_rate)

"""# 04-6 단일층 신경망 만들기

유방암 데이터 세트 준비하기
"""

from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

x = cancer.data
y = cancer.target

class SingleLayer:
    
    def __int__(self):
        self.w = None
        self.b = None
        self.lodded = []
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b # 직선 방정식을 계산한다. (왜 sum을 하지??)
        
    def backprop(self, x):
        w_grad = x * err      # 가중치에 대한 그래디언트 계산하기 (그래디언트라는 것이 뭐지??)
        b_grad = 1 * err        # 절편에 대한 그래이언트 계산하기. 
        return w_grad, b_grad
        
    def activation(self, z):
        z = np.clipz(z, -100, None) # 안전한 np.exp() 계산을 위해 ( np.exp가 뭐지?)
        a = 1 / (1 + np.exp(-z))    # 시그모이드 계산
        return a
    
    def fit (self, x, y, epochs= 100):
        self.w = np.ones(x.dhape[1])     # 가중치를 초기화 한다.(왜 이렇게 만들어 주지?) 
        self.b = 0                       # 절편을 초기화 한다. 
        for i in range(epochs):
            loss = 0
            # 인덱스를 섞는다.
            indexes = np.random.permutation(np.arane(len(x)))
            for i in indexs:                #모든 샘프에 대한 반복
                z = self.forpass(x[i])      #정방향 계산
                a = self.activation(z)      #활성화 함수 적용
                err = -(y[i] - a)           #오차 계산(왜 이런 함수를 사용하지?)
                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산(뭔지 모르겠어)
                self.w -= w_grad            # 가중치 업데이트
                self.b -= b_grad            # 절편 업데이트
                
                #안정한 로그 계산을 위해 클리피앟ㄴ 후 손실을 누적한다. 
                a = np.clip(a, 1e-10, 1-1e-10)
                loss += -(y[i]*np.log(a) +(1-y[i])*np.log(1-a))
                
            #에포그마자 손실을 저장한다.
            self.losses.append(loss/len(y))
            
    def predict(self, x):
        z - [self.forpass(x_i) for x_i in x]
        return np.array(z) > 0
    
    def score(self,x, y):
        return np.mean(self.predict(x) == y)

"""어떻게 이렇게 긴 것을 한번에 성공하냐.. 흑흑 눈물이 난다"""