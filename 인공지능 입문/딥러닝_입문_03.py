# -*- coding: utf-8 -*-
"""딥러닝 입문 03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UW84K8zO8o6dUTYFvOstEwF3Xp5LPyj7

# 03 머신러닝의 기초를 다집니다.

1차 함수로 이해하는 선형회귀

# 03-1 선형 회귀에 대해 알아봅니다.
"""

from sklearn.datasets import load_diabetes
diabetes = load_diabetes()                 #()인가요?

diabetes

print(diabetes.data.shape, diabetes.target.shape)

diabetes.data[0:3]

diabetes.data[0:3, :]

diabetes.target[:3]

"""당뇨병 환자 데이터 시각화하기"""

# 1. 맷플롯립의 scatter()함수로 산점도 그리기
import matplotlib.pyplot as plt

plt.scatter(diabetes.data[:, 2], diabetes.target) #인덱스 값이 2번인 녀석을 가지고 와

plt.scatter(diabetes.data[:, 2], diabetes.target[:]) # 벡터는 차원이 없다. ( 벡터 = 1차원 행렬)

x = diabetes.data[:, 2] # [:, :2] ':'이 없을때는  숫자 번호의 인덱스데이터를 가지고 오세요
y = diabetes.target

"""<퀴즈> 슬라이싱을 사용해 훈련 데이터 x에서 100번째 부터 109번째 까지 샘플을 출력해 보세요. 슬라이싱한 배열의 크기는 얼마인가요?"""

x[100:110]

# 99번째부터 108번까지의 샘플을 가지고 오세요
x_sample = x[99:109,] 
print(x_sample, x_sample.shape)

# 99번째부터 108번까지의 샘플을 가지고 오세요
x_sample = x[99:109] #(10, 1)
print(x_sample, x_sample.shape)

x_sample = x[99:109][2] #(10, ) 3번째 값을 가지고 오세요 : 2번째 인덱스의 값을 가지고 오세요
print(x_sample, x_sample.shape)

#인덱스를 가지고 오세요
# vs 인덱스의 값을 가지고 오세요

"""## 경사하강법에 대해 알아봅니다.

예측값과 변화율에 대해 알아봅니다.

1. 무작위로 w와 b를 정한다.(무작위로 모델 만들기)
2. x에서 샘플 하나를 선택하여 예측값을 계산한다.(무작위로 모델 만들기)
3. 예측갖과 선택한 샘플의 진짜 y값을 비교합니다.
4. 예측값이 y와 더 가까워지도록 w, b를 조정한다.(모델 조정하기) 
5. 모든 샘플을 처리할 때까지 다시 2 ~ 4 항목을 반복한다.

w(값 조정한 후에)예측값 증가 정도 확인하기

# 경사하강법을 배우는 목적 :  경사하강법을 통해 선형회귀모델을 찾기위함이다. (선형회귀)

![](../Data/GradientDescent01.gif)
"""

# 1. 임의의 값으로 w와 b초기화 하기
w = 1.0
b = 1.0

# 2. - 첫번째 인덱스 값 (0번) 샘플에 대한 예측 만들기
y_hat = x[0]*w + b
print(y_hat)

# 3.예측갖과 선택한 샘플의 진짜 y값을 비교합니다.
print(y[0])

# 4. 예측값이 y와 더 가까워지도록 w, b를 조정한다.(모델 조정하기)
w_inc = w + 0.1
y_hat_inc = w_inc * x[0] + b
print(y_hat_inc)

w_rate = (y_hat_inc - y_hat) / (w_inc - w) #변화율을 왜 구해?
print(w_rate)

"""## 변화율로 가중치 업데이트하기"""

# 변화율로 가중치를 업데이트하는 방법
w_new = w + w_rate
print(w_new)

"""## 변화율로 절편 업데이트하기"""

b_inc = b + 0.1           #  b를 0.1만큼 증가시켜 봅니다.
y_hat_inc = x[0]*w +b_inc  
print(y_hat_inc)

b_rate = (y_hat_inc - y_hat) / (b_inc - b) # b의 변화율
print(b_rate)

# 에게 왜 있는 것인가??? -> 의미 없어
b_new = b + 1
print(b_new)

"""이 방식에 어떤 문제접이 있까요?
1. y_hat이 y에 한참 미치지 못하는 값인 경우, w와 b를 더 큰 폭으수정할 수 없다. 
2. y_hat이 y보다 커지면 y_hat을 감소시키지 못 한다.

# 오차 역전파로 가중치와 절편을 업데이트 한다.
"""

# 1. 오차 값와 변화율을 곱하여 가중치 업데이트하기(+, -)
err = y[0] - y_hat  # 오차 값
w_new = w + w_rate * err 
b_new = b + 1 * err # b_new = b + b_rate * err
print(w_new, b_new)

"""### 두번째 샘플을 사용하여 w와 b를 계산하기"""

y_hat = x[1] * w + b # 두번째 : 1 인덱스 값
err = y[1] - y_hat
w_rate = x[1]                 # 두 번째 샘플의 변화율은 샘플 값 그 자체 
w_new = w_new + w_rate * err 
b_new = b_new + 1 * err
print(w_new, b_new)

"""### 전체 샘플을 한 번 반복하여 가중치와 절편을 조정하기"""

# 3. 전체 샘플 반복하기
for x_i, y_i in zip(x, y):   #zip 압축 range()
    y_hat = x_i*w + b
    err = y_i - y_hat
    w_rate = x_i
    w = w + w_rate * err
    b = b + 1 * err
print(w, b)

plt.scatter(x, y) 

pt1 = (-0.1, -0.1 * w + b) #(x 값 : y 예측값)
pt2 = (0.15, 0.15 * w +b )
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])  # plt.plot(pt1, pt2)

plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""### 전체 샘플을  백 번 반복하여 가중치와 절편을 조정하기"""

for i in range(1, 100):   # 99번 돌리세요 epoch
    for x_i, y_i in zip(x, y):
        y_hat = x_i * w + b
        err = y_i - y_hat
        w_rate = x_i
        w = w + w_rate * err
        b = b + 1 * err
print(w, b)

epochs = 100
for i in range(epochs):   # 100번 돌리세요 epoch(에포크)
    for x_i, y_i in zip(x, y):
        y_hat = x_i * w + b
        err = y_i - y_hat
        w_rate = x_i
        w = w + w_rate * err
        b = b + 1 * err
print(w, b)

"""경사 하강법으로 찾은 선형 회귀 모델

y = 913.6x + 123.4
"""

plt.scatter(x, y)
pt1 = (-0.1, -0.1 * w + b) # -0.1인 이유가 있나? 
pt2 = (0.15, 0.15 * w + b) # 0.15인 의미가 있나?  -> 왜 구지 숫자를 지정해 주었을까?
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

plt.scatter(x, y)
pt1 = (min(x), min(x) * w + b) # -0.1인 이유가 있나? 
pt2 = (max(x), max(x) * w + b) # 0.15인 의미가 있나?  -> 왜 구지 숫자를 지정해 주었을까?
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

plt.scatter(x, y)
pt1 = (x[0], x[0] * w + b) # -0.1인 이유가 있나? 
pt2 = (x[441], x[441] * w + b) # 0.15인 의미가 있나?  -> 왜 구지 숫자를 지정해 주었을까?
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], c='r')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""# 모델로 예측하기"""

# 6. 모델 예측하기
x_new = 0.18           # 임의의 값 x
y_pred = x_new * w + b # 임의의 값 x의 예측값 확인하기
print(y_pred)          # 의미 없어

plt.scatter(x, y)
plt.scatter(x_new, y_pred)
pt1 = (min(x), min(x) * w + b)
pt2 = (0.19, 0.19 * w + b)
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""# 03-3 손실 함수와 경사 하강법의 관계를 알아 봅시다.

1. 손실 함수는 예측한 값과 실제 타깃값의 차이를 측정한다.
2. 손실 함수의 값을 줄이기 위해서는 경사하강법을 사용한다.

손실함수값이 제곱인 이유는 큰 값 일수록 큰 변화를 주고 낮은 값일 수록 변화를 작게 주기 위함이다.

## 미분하기

목적 : 기울기를 찾아서
"""

y_hat = x_i * w + b
err = y_i - y_hat
w_rate = x_i
w = w + w_rate * err

"""# 절편에 대하여 제곱오차 미분하기

# 03-4 뉴런을 만들어봅니다.

Neuron 클래스 만들기

class Neuron:
    def __init__(self): #def : define, init : initiate의 줄임말
     
    # 초기화작업 수행
    
    # 필요한 메서드 추가

# init() 메서드 만들기
- 가중치와 절편을 설정

```python
def __init__(self):
    self.w = 1.0 #가중치를 초기화
    self.b = 1.0 #절편을 초기화 
```

정방향 계산 만들기 (순정파, 피드 포위드(feed forward))

```python
def forpass(self, x): # for :앞으로, pass:전달하다
    y_hat = x * self.w + self.b
    return y_hat
```

역방향 계산 만들기(역전파 , 백 프로파게이션(back propagation))
- 오차값 계산하기
- 오차값 가중치 업데이트
- 오차값 절편 업데이트

```python
def backprop(self, x, err):
    w_grad = x * err
    b_grad = 1 * err
    return w_grad, b_grawd
```

훈련을 위한 fit()메서드 구현

```python
def fit(self, x, y, epochs=100):
    for i in range(epochs):
        for x_i, y_i in zip(x, y):
            y_hat = self.forpass(x_i)
            err = -(y_i - y_hat)
            w_grad , b_grad = self.backprop(x_i, err)
            self.w -= w_grad
            self.b -= b_grad
```
"""

class Neuron:
    
    def __init__(self):
        self.w = 1.0
        self.b = 1.0
        
    def forpass(self, x):
        y_hat = x * self.w + self.b
        return y_hat
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    def fit(self, x, y, epochs=100):
        for i in range(epochs):
                for x_i, y_i in zip(x, y):
                    y_hat = self.forpass(x_i)
                    err = -(y_i - y_hat)
                    w_grad, b_grad = self.backprop(x_i, err)
                    self.w -= w_grad
                    self.b -= b_grad

"""혼자 만드는 것을 해보세요
python 클래스 함수

# 뉴런 훈련
"""

neuron = Neuron()
neuron.fit(x, y)

plt.scatter(x, y)
pt1 = (-0.1, -0.1 * neuron.w + neuron.b)
pt2 = (0.15, 0.15 * neuron.w + neuron.b)
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

